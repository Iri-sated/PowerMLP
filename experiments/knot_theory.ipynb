{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import set_seed\n",
    "import numpy as np\n",
    "from grid_search import BaseSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "set_seed(seed)\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "device = torch.device('cuda:2')\n",
    "\n",
    "file_name = '../data/knot.csv'\n",
    "full_df = pd.read_csv(file_name)\n",
    "\n",
    "display_name_from_short_name = {\n",
    "    'chern_simons': 'Chern-Simons',\n",
    "    'cusp_volume': 'Cusp volume',\n",
    "    'hyperbolic_adjoint_torsion_degree': 'Adjoint Torsion Degree',\n",
    "    'hyperbolic_torsion_degree': 'Torsion Degree',\n",
    "    'injectivity_radius': 'Injectivity radius',\n",
    "    'longitudinal_translation': 'Longitudinal translation',\n",
    "    'meridinal_translation_imag': 'Re(Meridional translation)',\n",
    "    'meridinal_translation_real': 'Im(Meridional translation)',\n",
    "    'short_geodesic_imag_part': 'Im(Short geodesic)',\n",
    "    'short_geodesic_real_part': 'Re(Short geodesic)',\n",
    "    'Symmetry_0': 'Symmetry: $0$',\n",
    "    'Symmetry_D3': 'Symmetry: $D_3$',\n",
    "    'Symmetry_D4': 'Symmetry: $D_4$',\n",
    "    'Symmetry_D6': 'Symmetry: $D_6$',\n",
    "    'Symmetry_D8': 'Symmetry: $D_8$',\n",
    "    'Symmetry_Z/2 + Z/2': 'Symmetry: $\\\\frac{Z}{2} + \\\\frac{Z}{2}$',\n",
    "    'volume': 'Volume',\n",
    "}\n",
    "column_names = list(display_name_from_short_name)\n",
    "target = 'signature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(seed)\n",
    "train_df, validation_and_test_df = train_test_split(\n",
    "    full_df, random_state=random_state)\n",
    "validation_df, test_df = train_test_split(\n",
    "    validation_and_test_df, test_size=.5, random_state=random_state)\n",
    "\n",
    "# Find bounds for the signature in the training dataset.\n",
    "max_signature = train_df[target].max()\n",
    "min_signature = train_df[target].min()\n",
    "classes = int((max_signature - min_signature) / 2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df, cols, add_target=True):\n",
    "    features = df[cols]\n",
    "    sigma = features.std()\n",
    "    if any(sigma == 0):\n",
    "        print(sigma)\n",
    "        raise RuntimeError(\n",
    "            \"A poor data stratification has led to no variation in one of the data \"\n",
    "            \"splits for at least one feature (ie std=0). Restratify and try again.\")\n",
    "    mu = features.mean()\n",
    "    normed_df = (features - mu) / sigma\n",
    "    if add_target:\n",
    "        normed_df[target] = df[target]\n",
    "    return normed_df\n",
    "\n",
    "\n",
    "def get_batch(df, cols, size=None):\n",
    "    batch_df = df if size is None else df.sample(size)\n",
    "    X = batch_df[cols].to_numpy()\n",
    "    y = batch_df[target].to_numpy()\n",
    "    y = torch.tensor(y)\n",
    "    y = (y - torch.ones(len(y)) * min_signature)/2\n",
    "    y = y.long()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "normed_train_df = normalize_features(train_df, column_names)\n",
    "normed_validation_df = normalize_features(validation_df, column_names)\n",
    "normed_test_df = normalize_features(test_df, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import KANDataset\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "train_X, train_y = get_batch(normed_train_df, column_names)\n",
    "val_X, val_y = get_batch(normed_validation_df, column_names)\n",
    "test_X, test_y = get_batch(normed_test_df, column_names)\n",
    "\n",
    "train_X = torch.tensor(train_X).float()\n",
    "val_X = torch.tensor(val_X).float()\n",
    "test_X = torch.tensor(test_X).float()\n",
    "\n",
    "trainset = KANDataset(train_X, train_y)\n",
    "valset = KANDataset(val_X, val_y)\n",
    "testset = KANDataset(test_X, test_y)\n",
    "    \n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_list = [\n",
    "    [2,4,1],[2,8,1],[2,16,1],[2,32,1],\n",
    "    [2,4,4,1],[2,8,8,1],[2,16,16,1],[2,32,32,1],\n",
    "    [2,4,4,4,1],[2,8,8,8,1],[2,16,16,16,1],[2,32,32,32,1],\n",
    "    [2,4,4,4,4,1],[2,8,8,8,8,1],[2,16,16,16,16,1],[2,32,32,32,32,1],\n",
    "]    \n",
    "lr_list = [0.3,0.2,0.1,0.08,0.05,0.03,0.01,0.008,0.005,0.001]\n",
    "\n",
    "gs = BaseSearcher(device=device,save_dir='save/knots/')\n",
    "gs.init_logs()\n",
    "gs.grid_search(size_list,lr_list,train_loader,val_loader,test_loader,repu_order=3,optim='lbfgs',max_iter=500,epoch_list=[100,100,500],scheduler='cos')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
